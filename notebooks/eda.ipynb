{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# eBay NER Dataset - Exploratory Data Analysis\n",
        "\n",
        "This notebook provides comprehensive analysis of the eBay German NER dataset to inform preprocessing decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src.data.load_data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Add src to path for imports\u001b[39;00m\n\u001b[1;32m     11\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../src\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_tagged_train, read_listings, to_bio_sequences, load_label_list, build_bio_maps\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Set up plotting\u001b[39;00m\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.data.load_data'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append('../src')\n",
        "from data.load_data import read_tagged_train, read_listings, to_bio_sequences, load_label_list, build_bio_maps\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Basic Dataset Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "tagged_df = read_tagged_train('../data/Tagged_Titles_Train.tsv.gz')\n",
        "listings_df = read_listings('../data/Listing_Titles.tsv.gz')\n",
        "\n",
        "print(\"=== Dataset Overview ===\")\n",
        "print(f\"Tagged training records: {tagged_df['Record Number'].nunique()}\")\n",
        "print(f\"Total tokens in training: {len(tagged_df)}\")\n",
        "print(f\"Listings records: {len(listings_df)}\")\n",
        "print(f\"Categories in training: {tagged_df['Category'].nunique()}\")\n",
        "print(f\"Unique tokens: {tagged_df['Token'].nunique()}\")\n",
        "\n",
        "print(\"\\n=== Training Data Sample ===\")\n",
        "print(tagged_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Title Length Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze title lengths\n",
        "title_lengths = tagged_df.groupby('Record Number')['Token'].count()\n",
        "\n",
        "print(\"=== Title Length Statistics ===\")\n",
        "print(f\"Mean tokens per title: {title_lengths.mean():.2f}\")\n",
        "print(f\"Median tokens per title: {title_lengths.median():.2f}\")\n",
        "print(f\"Max tokens per title: {title_lengths.max()}\")\n",
        "print(f\"Min tokens per title: {title_lengths.min()}\")\n",
        "print(f\"95th percentile: {title_lengths.quantile(0.95):.2f}\")\n",
        "print(f\"99th percentile: {title_lengths.quantile(0.99):.2f}\")\n",
        "\n",
        "# Plot title length distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax1.hist(title_lengths, bins=50, alpha=0.7, edgecolor='black')\n",
        "ax1.set_xlabel('Tokens per Title')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Distribution of Title Lengths')\n",
        "ax1.axvline(title_lengths.mean(), color='red', linestyle='--', label=f'Mean: {title_lengths.mean():.1f}')\n",
        "ax1.axvline(160, color='orange', linestyle='--', label='Config Max: 160')\n",
        "ax1.legend()\n",
        "\n",
        "# Box plot\n",
        "ax2.boxplot(title_lengths)\n",
        "ax2.set_ylabel('Tokens per Title')\n",
        "ax2.set_title('Title Length Box Plot')\n",
        "ax2.axhline(160, color='orange', linestyle='--', label='Config Max: 160')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Titles that exceed max_length\n",
        "long_titles = title_lengths[title_lengths > 160]\n",
        "print(f\"\\nTitles exceeding max_length (160): {len(long_titles)} ({len(long_titles)/len(title_lengths)*100:.2f}%)\")\n",
        "if len(long_titles) > 0:\n",
        "    print(f\"Longest title: {long_titles.max()} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Label Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load labels and create BIO mapping\n",
        "labels = load_label_list('../configs/labels.txt')\n",
        "bio_labels, id2label, label2id = build_bio_maps(labels)\n",
        "\n",
        "print(f\"=== Label Analysis ===\")\n",
        "print(f\"Original labels: {len(labels)}\")\n",
        "print(f\"BIO labels: {len(bio_labels)}\")\n",
        "print(f\"Labels: {labels}\")\n",
        "\n",
        "# Analyze tag distribution\n",
        "tag_counts = tagged_df['Tag'].value_counts()\n",
        "print(f\"\\n=== Tag Distribution ===\")\n",
        "print(f\"Most common tags:\")\n",
        "print(tag_counts.head(10))\n",
        "\n",
        "print(f\"\\nLeast common tags:\")\n",
        "print(tag_counts.tail(10))\n",
        "\n",
        "# Plot tag distribution\n",
        "plt.figure(figsize=(15, 8))\n",
        "tag_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Tags in Training Data')\n",
        "plt.xlabel('Tag')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate label imbalance\n",
        "total_tokens = len(tagged_df)\n",
        "o_count = tag_counts.get('O', 0)\n",
        "non_o_count = total_tokens - o_count\n",
        "\n",
        "print(f\"\\n=== Label Imbalance ===\")\n",
        "print(f\"O (Outside) tokens: {o_count} ({o_count/total_tokens*100:.2f}%)\")\n",
        "print(f\"Named entity tokens: {non_o_count} ({non_o_count/total_tokens*100:.2f}%)\")\n",
        "print(f\"Imbalance ratio: {o_count/non_o_count:.2f}:1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Category Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze category distribution\n",
        "category_counts = tagged_df.groupby('Record Number')['Category'].first().value_counts()\n",
        "\n",
        "print(\"=== Category Distribution ===\")\n",
        "print(category_counts)\n",
        "\n",
        "# Plot category distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "category_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Categories')\n",
        "plt.xlabel('Category ID')\n",
        "plt.ylabel('Number of Records')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze label distribution by category\n",
        "print(\"\\n=== Labels by Category ===\")\n",
        "for cat in sorted(tagged_df['Category'].unique()):\n",
        "    cat_data = tagged_df[tagged_df['Category'] == cat]\n",
        "    cat_tags = cat_data['Tag'].value_counts()\n",
        "    print(f\"\\nCategory {cat} (n={len(cat_data.groupby('Record Number'))} records):\")\n",
        "    print(cat_tags.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Character and Token Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character-level analysis\n",
        "all_tokens = tagged_df['Token'].tolist()\n",
        "all_text = ' '.join(all_tokens)\n",
        "\n",
        "print(\"=== Character Analysis ===\")\n",
        "print(f\"Total characters: {len(all_text)}\")\n",
        "print(f\"Unique characters: {len(set(all_text))}\")\n",
        "print(f\"German umlauts (äöüß): {sum(1 for c in all_text if c in 'äöüß')}\")\n",
        "print(f\"Numbers: {sum(1 for c in all_text if c.isdigit())}\")\n",
        "print(f\"Special chars: {sum(1 for c in all_text if not c.isalnum() and c not in 'äöüß')}\")\n",
        "\n",
        "# Token length analysis\n",
        "token_lengths = [len(token) for token in all_tokens]\n",
        "print(f\"\\n=== Token Length Analysis ===\")\n",
        "print(f\"Mean token length: {np.mean(token_lengths):.2f}\")\n",
        "print(f\"Max token length: {max(token_lengths)}\")\n",
        "print(f\"Tokens with special chars: {sum(1 for t in all_tokens if not t.replace('-', '').replace('_', '').isalnum())}\")\n",
        "\n",
        "# Plot token length distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(token_lengths, bins=30, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Characters per Token')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Token Length Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(token_lengths, bins=30, alpha=0.7, edgecolor='black', cumulative=True, density=True)\n",
        "plt.xlabel('Characters per Token')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.title('Token Length CDF')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. BIO Sequence Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to BIO sequences for analysis\n",
        "bio_sequences = to_bio_sequences(tagged_df)\n",
        "\n",
        "print(f\"=== BIO Sequence Analysis ===\")\n",
        "print(f\"Total sequences: {len(bio_sequences)}\")\n",
        "\n",
        "# Analyze sequence lengths\n",
        "seq_lengths = [len(seq['tokens']) for seq in bio_sequences]\n",
        "print(f\"Mean sequence length: {np.mean(seq_lengths):.2f}\")\n",
        "print(f\"Max sequence length: {max(seq_lengths)}\")\n",
        "\n",
        "# Analyze label patterns\n",
        "all_bio_labels = []\n",
        "for seq in bio_sequences:\n",
        "    all_bio_labels.extend(seq['bio_labels'])\n",
        "\n",
        "bio_counts = Counter(all_bio_labels)\n",
        "print(f\"\\n=== BIO Label Distribution ===\")\n",
        "print(f\"Most common BIO labels:\")\n",
        "for label, count in bio_counts.most_common(10):\n",
        "    print(f\"  {label}: {count}\")\n",
        "\n",
        "# Analyze entity spans\n",
        "entity_spans = []\n",
        "for seq in bio_sequences:\n",
        "    tokens = seq['tokens']\n",
        "    labels = seq['bio_labels']\n",
        "    \n",
        "    current_span = []\n",
        "    for i, (token, label) in enumerate(zip(tokens, labels)):\n",
        "        if label.startswith('B-'):\n",
        "            if current_span:\n",
        "                entity_spans.append(len(current_span))\n",
        "            current_span = [token]\n",
        "        elif label.startswith('I-'):\n",
        "            current_span.append(token)\n",
        "        else:  # O\n",
        "            if current_span:\n",
        "                entity_spans.append(len(current_span))\n",
        "                current_span = []\n",
        "    if current_span:\n",
        "        entity_spans.append(len(current_span))\n",
        "\n",
        "if entity_spans:\n",
        "    print(f\"\\n=== Entity Span Analysis ===\")\n",
        "    print(f\"Total entity spans: {len(entity_spans)}\")\n",
        "    print(f\"Mean span length: {np.mean(entity_spans):.2f}\")\n",
        "    print(f\"Max span length: {max(entity_spans)}\")\n",
        "    print(f\"Spans of length 1: {sum(1 for s in entity_spans if s == 1)} ({sum(1 for s in entity_spans if s == 1)/len(entity_spans)*100:.1f}%)\")\n",
        "    print(f\"Spans of length >5: {sum(1 for s in entity_spans if s > 5)} ({sum(1 for s in entity_spans if s > 5)/len(entity_spans)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Quality Assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for data quality issues\n",
        "print(\"=== Data Quality Assessment ===\")\n",
        "\n",
        "# Missing values\n",
        "print(f\"Missing values per column:\")\n",
        "print(tagged_df.isnull().sum())\n",
        "\n",
        "# Empty tokens\n",
        "empty_tokens = tagged_df['Token'].str.strip() == ''\n",
        "print(f\"\\nEmpty tokens: {empty_tokens.sum()}\")\n",
        "\n",
        "# Empty tags (continuation tags)\n",
        "empty_tags = tagged_df['Tag'].str.strip() == ''\n",
        "print(f\"Empty tags (continuation): {empty_tags.sum()}\")\n",
        "\n",
        "# Inconsistent record lengths\n",
        "record_lengths = tagged_df.groupby('Record Number').size()\n",
        "inconsistent_records = record_lengths[record_lengths != record_lengths.mode().iloc[0]]\n",
        "print(f\"\\nRecords with inconsistent token counts: {len(inconsistent_records)}\")\n",
        "\n",
        "# Check for potential annotation errors\n",
        "print(f\"\\n=== Potential Issues ===\")\n",
        "print(f\"Records with no named entities: {sum(1 for seq in bio_sequences if all(label == 'O' for label in seq['bio_labels']))}\")\n",
        "print(f\"Records with only named entities: {sum(1 for seq in bio_sequences if all(label != 'O' for label in seq['bio_labels']))}\")\n",
        "\n",
        "# Sample problematic cases\n",
        "print(f\"\\n=== Sample Records ===\")\n",
        "for i, seq in enumerate(bio_sequences[:3]):\n",
        "    print(f\"\\nRecord {i+1}:\")\n",
        "    print(f\"  Tokens: {seq['tokens'][:10]}{'...' if len(seq['tokens']) > 10 else ''}\")\n",
        "    print(f\"  Labels: {seq['bio_labels'][:10]}{'...' if len(seq['bio_labels']) > 10 else ''}\")\n",
        "    print(f\"  Length: {len(seq['tokens'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Preprocessing Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Preprocessing Recommendations ===\")\n",
        "\n",
        "# Max length recommendation\n",
        "p95_length = title_lengths.quantile(0.95)\n",
        "p99_length = title_lengths.quantile(0.99)\n",
        "config_max = 160\n",
        "\n",
        "print(f\"Current config max_length: {config_max}\")\n",
        "print(f\"95th percentile: {p95_length:.1f}\")\n",
        "print(f\"99th percentile: {p99_length:.1f}\")\n",
        "print(f\"Titles exceeding config: {sum(title_lengths > config_max)} ({sum(title_lengths > config_max)/len(title_lengths)*100:.2f}%)\")\n",
        "\n",
        "if p95_length <= config_max:\n",
        "    print(f\"✓ Config max_length={config_max} covers 95% of data\")\n",
        "else:\n",
        "    print(f\"⚠ Consider increasing max_length to {int(p95_length)} to cover 95% of data\")\n",
        "\n",
        "# Label imbalance\n",
        "imbalance_ratio = o_count / non_o_count\n",
        "print(f\"\\nLabel imbalance ratio: {imbalance_ratio:.2f}:1 (O:NE)\")\n",
        "if imbalance_ratio > 10:\n",
        "    print(\"⚠ High label imbalance - consider class weights\")\n",
        "else:\n",
        "    print(\"✓ Label imbalance is manageable\")\n",
        "\n",
        "# Rare labels\n",
        "rare_labels = tag_counts[tag_counts < 10]\n",
        "print(f\"\\nRare labels (<10 occurrences): {len(rare_labels)}\")\n",
        "if len(rare_labels) > 0:\n",
        "    print(f\"Rare labels: {list(rare_labels.index)}\")\n",
        "    print(\"⚠ Consider grouping rare labels or using class weights\")\n",
        "\n",
        "# Train/val split recommendation\n",
        "total_records = len(bio_sequences)\n",
        "print(f\"\\nTotal records for training: {total_records}\")\n",
        "print(f\"Recommended train/val split: {int(total_records * 0.9)}/{int(total_records * 0.1)}\")\n",
        "print(f\"Records per category in validation: ~{int(total_records * 0.1 / len(category_counts))}\")\n",
        "\n",
        "print(f\"\\n=== Summary ===\")\n",
        "print(f\"✓ Dataset size: {total_records} records, {len(tagged_df)} tokens\")\n",
        "print(f\"✓ Label vocabulary: {len(bio_labels)} BIO labels\")\n",
        "print(f\"✓ Max sequence length: {max(seq_lengths)} tokens\")\n",
        "print(f\"✓ Categories: {len(category_counts)} (balanced: {len(category_counts) == 2})\")\n",
        "print(f\"✓ Ready for preprocessing pipeline\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
